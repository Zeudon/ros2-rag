{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from clearml import Task, PipelineController\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, PointStruct, Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task.init(project_name=\"Featurization Pipeline\", task_name=\"Featurization Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1229\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to the MongoDB server\n",
    "client = MongoClient(\"mongodb://mongoadmin:secret@localhost:27017/\")  # Replace with your connection string\n",
    "\n",
    "# Access the database and collection\n",
    "db = client[\"media_data_final2\"]\n",
    "collection = db[\"raw_data_final2\"]\n",
    "\n",
    "# Retrieve all records\n",
    "records = collection.find()\n",
    "count = collection.count_documents({})\n",
    "print(count)\n",
    "# Iterate through and print each record\n",
    "# for record in records:\n",
    "#     print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to MongoDB\n",
    "\n",
    "# MongoDB Configuration\n",
    "MONGO_URI = \"mongodb://mongoadmin:secret@localhost:27017/\"\n",
    "DB_NAME = \"media_data_final\"\n",
    "EMBEDDINGS_COLLECTION_NAME = \"embeddings\"\n",
    "RAW_DATA_COLLECTION_NAME = \"raw_data_final2\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "db = mongo_client[DB_NAME]\n",
    "collection = db[RAW_DATA_COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Qdrant\n",
    "\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"test_collection\"\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=\"Cosine\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_and_persist(data, source, path):\n",
    "    mongo_client = MongoClient(MONGO_URI)\n",
    "    db = mongo_client[DB_NAME]\n",
    "    collection = db[EMBEDDINGS_COLLECTION_NAME]\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2') # TODO: Pick better models\n",
    "    embeddings = model.encode(data)\n",
    "    \n",
    "    document = {\n",
    "        \"source\": source,\n",
    "        \"path\": path,\n",
    "        \"embeddings\": embeddings.tolist(),\n",
    "        \"data\": data\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "    print(\"Embeddings stored into MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_youtube_data(data):\n",
    "    sentences = [i['text'] for i in data]\n",
    "    chunks = \" \".join(sentences)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_embeddings():\n",
    "    client = MongoClient(\"mongodb://mongoadmin:secret@localhost:27017/\")  # Replace with your connection string\n",
    "\n",
    "    # Access the database and collection\n",
    "    db = client[\"media_data_final2\"]\n",
    "    collection = db[\"raw_data_final2\"]\n",
    "\n",
    "    # Retrieve all records\n",
    "    records = collection.find()\n",
    "    count = 0\n",
    "    for record in records:\n",
    "        count+=1\n",
    "        source = record.get(\"source\")\n",
    "        if source == \"web\":\n",
    "            data = record.get(\"content\")\n",
    "        data = record.get(\"data\")\n",
    "        if (source == \"youtube\"):\n",
    "            data = chunk_youtube_data(record.get(\"data\"))\n",
    "        create_embeddings_and_persist(data, source, record.get(\"url\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":orphan:\n",
      ".. _summer_2021_student_program:\n",
      "Projects for 2021 Summer Student Program\n",
      "########################################\n",
      "The Summer 2021 Student Program <https://summer.iscas.ac.cn/help/en/student/>_ is upon us!\n",
      "See below for the list of project proposals for students to review and apply for.\n",
      "This is by no means the only list of potential projects, please check our issue tracker or propose another if there's something you think you're well suited for that would be useful for Nav2.\n",
      "If you have any questions, wish to ask questions, or generally reach out, you are encouraged to do so!\n",
      "We have a community Slack at navigation2.slack.com <https://navigation2.slack.com>_.\n",
      "If you are unable to access it due to not being able to sign up, please send an email to the mentor in the details below and they can manually add you.\n",
      "Additionally, we have ROS community Discourse.\n",
      "If you post on this discourse page <https://discourse.ros.org>_ regarding Nav2, a maintainer will respond.\n",
      "We have the sub-topic Navigation Stack that may be used here <https://discourse.ros.org/c/navigation/44>_.\n",
      "It is preferred you post in this sub-topic.\n",
      "For general ROS questions, please use ROS Answers <https://answers.ros.org/>_.\n",
      "Have fun and we look forward to working with you this summer!\n",
      ":maxdepth: 1\n",
      "projects/dynamic.rst\n",
      "projects/testing.rst\n",
      "projects/assisted_teleop.rst\n",
      "projects/multithreading.rst\n",
      "projects/safety_node.rst\n",
      "projects/semantics.rst\n",
      "projects/spinners.rst\n",
      "projects/twist_n_config.rst\n",
      ".. _assisted_teleop:\n",
      "3. Assisted Teleop\n",
      "**Task description**\n",
      "In mobile robot and autonomous vehicle navigation, there are situations where a human driver is required to intervene to get the vehicle out of a sticky situation. This can be both as a backup in case of autonomy failure as well as the primary function of the robot (e.g. telepresence robots).\n",
      "This project's aim is to create an assisted teleop feature in Nav2 by means of a new behavior tree configuration file (the file that defines the flow of information for the navigation task) and potentially new plugins. This feature should make sure to use the local costmap and/or sensor data in order to avoid obstacles and take position and/or velocity commands to attempt to follow.\n",
      "An example application of this is a telepresence robot, where a human driver is driving the robot through a space to visit in an office building or hospital. Another example would be an autonomous delivery robot stuck requiring a human driver to navigate it back into an open space for the robot to continue its task.\n",
      "This will be an excellent chance to make a substantial new feature in the Nav2 system to be used by hundreds of robots in the future. This project could also be a good candidate for a ROSCon talk in future events.\n",
      "**Project difficulty: Medium**\n",
      "**Project community mentor: Steve Macenski** @SteveMacenski <https://github.com/SteveMacenski>_\n",
      "**Mentor contact details: [See link above, link in GitHub profile description]**\n",
      "**Project output requirements**\n",
      "- Integrations of an assisted teleop feature in Nav2 as either a set of plugins and/or a behavior tree configuration\n",
      "- Robot can successfully navigate a space by a user teleop without collision\n",
      "- If time allots, work on tuning / adding new critics to the DWB local planner to improve safety of its performance for users out of the box\n",
      "**Skills required**\n",
      "- C++, XML, ROS\n",
      "- Mobile robot navigation experience\n",
      "- Recommended: Gazebo simulation, ROS navigation, Behavior trees\n",
      "**List of relevant open source software repositories and refs**\n",
      "- ROS <https://www.ros.org/>_\n",
      "- Gazebo Simulator <http://gazebosim.org/>_\n",
      "- Github ticket <https://github.com/ros-navigation/navigation2/issues/2226>_\n",
      "- Navigation2 <https://docs.nav2.org/>_\n",
      "**Licensing**\n",
      "- All contributions will be under the Apache 2.0 license.\n",
      "- No other CLA's are required.\n",
      ":orphan:\n",
      ".. _create_moveit_analog:\n",
      "1. Create a Configuration Assistant (Analog to MoveIt)\n",
      "**Task description**\n",
      "Moveit <https://moveit.ros.org/>_ has long has a QT configuration assistant <http://docs.ros.org/kinetic/api/moveit_tutorials/html/doc/setup_assistant/setup_assistant_tutorial.html>_. This setup assistant helps the user configure their UDRF and needs to setup MoveIt configuration files.\n",
      "A configuration assistant could be extremely beneficial to Navigation2 users as a way to minimize friction. We should provide a gui tool to cover the following configurations:\n",
      "- the broad strokes with the costmap, with a visualizer to show the user what it will look like\n",
      "- Select configurable costmap layers\n",
      "- Select recovery behavior parameters\n",
      "- URDF, footprint, and frame selection to make sure the options comply with standards, planner, and controller\n",
      "- Set minimum and maximum speed and other kinematic parameters\n",
      "- Select from a dropdown of possible planners and controllers\n",
      "- Helpful notes throughout the prompts to aid in selecting appropriate parameters\n",
      "- Selecting at behavior tree\n",
      "- @steve please add more specific options\n",
      "After the items are configured, there should be a preview to see how the parameters effect the robot.\n",
      "**Project difficulty: High**\n",
      "**Project community mentor: Steve Macenski** @SteveMacenski <https://github.com/SteveMacenski>_\n",
      "**Mentor contact details: [See link above, link in GitHub profile description]**\n",
      "**Contact information for the cooperating mentor (optional):  juzhenatpku@gmail.com**\n",
      "**Project output requirements**\n",
      "- A QT based GUI configuration assistant that support the parameters listed above\n",
      "- A preview panel to display the parameters' effection on the robot\n",
      "**Skills required**\n",
      "- C++, Python3, QT framework\n",
      "- JSON/XML parsing\n",
      "- 3D programming (maybe needed in the preview)\n",
      "- Recommended: Gazebo simulation, ROS, and Navigation experience\n",
      "**List of relevant open source software repositories and refs**\n",
      "- QT <https://www.qt.io/>_\n",
      "- Gazebo Simulator <http://gazebosim.org/>_\n",
      "- Original github issue page <https://github.com/ros-navigation/navigation2/issues/1721>_\n",
      "**Licensing**\n",
      "- All contributions will be under the Apache 2.0 license.\n",
      "- No other CLA's are required.\n",
      ":orphan:\n",
      ".. _create_plugins:\n",
      "2. Create New Planner and Controller Plugins\n",
      "**Task description**\n",
      "The ROS 2 Navigation Stack has a number of plugin interfaces to help users create or select specific plugins for planning, control, and behaviors for their applications. Two specific areas that the Nav2 stack could use more algorithm plugins for is for path planning (referred to as a planner plugin) and local trajectory generation (referred to as controller plugins). A simple tutorial for creating a planner plugin can be found here. <https://docs.nav2.org/tutorials/docs/writing_new_nav2planner_plugin.html>_ Currently, we have one planner, NavFn which implements an A* and Dijkstra's planner. It also has two controllers, DWB and TEB which implement a DWA and timed elastic-band optimization techniques. There is also a Hybrid-A* and OMPL planner in development.\n",
      "Your task will be to create a high-quality implementation of one of the following algorithms for the Nav2 plugin interfaces. Alternative algorithms may also be considered upon approval, please ask @steve in the application phase. Please select only one to discuss.\n",
      "- Planner Plugin Options: D* or variant, Vornoi planner, Navigation graph route planner, State Lattice planner, kinodynamic planner, and any planning method given a set of dynamic and static obstacles.\n",
      "- Controller Plugin Options: CiLQR, iLQR, MPC, Splines, path following or dynamic obstacle following controllers.\n",
      "- Additional options: helping in completing the OMPL or Hybrid-A* planner.\n",
      "**Project difficulty: High**\n",
      "**Project community mentor: Steve Macenski** @SteveMacenski <https://github.com/SteveMacenski>_\n",
      "**Mentor contact details: [See link above, link in GitHub profile description]**\n",
      "**Project output requirements**\n",
      "- A functional planner or controller plugin for the Nav2 stack\n",
      "- Plugin should be optimized for run-time performance with 50% or greater test coverage\n",
      "**Skills required**\n",
      "- C++\n",
      "- Path planning or motion planning\n",
      "- Algorithm optimization\n",
      "- ROS / Pluginlib\n",
      "- Recommended: Gazebo simulation and Navigation experience\n",
      "**List of relevant open source software repositories and refs**\n",
      "- ROS <https://www.ros.org/>_\n",
      "- Gazebo Simulator <http://gazebosim.org/>_\n",
      "- Github issue page <https://github.com/ros-navigation/navigation2/issues/1710>_\n",
      "- Nav2 <https://docs.nav2.org/>_\n",
      "**Licensing**\n",
      "- All contributions will be under the Apache 2.0 license.\n",
      "- No other CLA's are required.\n",
      ".. _dynamic:\n",
      "1. Navigation Dynamic Obstacle Integration\n",
      "**Task description**\n",
      "The Navigation Stack has long provided robust navigation in a wide range of environments. Controllers have been developed to operate effectively in the presence of dynamic obstacles without explicitly modeling the characteristics of dynamic obstacles. However, as the field has progressed and we see more and more robots using ROS deployed in human-filled spaces, more consideration must be taken with respect to dynamic obstacles such as people, carts, animals, and vehicles.\n",
      "Your task will be to create integrations with existing machine learning tools that create dynamic obstacle information (ComplexYolo, Yolo3D, etc) and tie them into the navigation stack for use. It is not in the scope for you to retrain or otherwise become an expert in 3D machine learning, but some basic knowledge will be helpful. We already have a starting point in the project links below that needs to be driven to completion. This includes completing the on-going work to integrate yolact edge into this work to replace detectron2 and benchmark these capabilities on GPUs to verify sufficient run-time performance, as well as other tangental feature development.\n",
      "This task will involve identifying a few techniques that produce position and velocity information about dynamic obstacles that can run on a mobile robot (using high-power Intel CPU, Nvidia Jetson SoC, external GPUs, etc) and get them running with ROS and Navigation. Next, you will help create a new costmap layer to use this information to mark the dynamic obstacle in the costmap to ensure a robot does not collide with a future trajectory of an obstacle.\n",
      "If time permits, you may also work to also integrate this dynamic information into a path planner and/or controller to help in direct motion consideration. This will likely be in collaboration with another community member.\n",
      "**Project difficulty: High**\n",
      "**Project community mentor: Steve Macenski** @SteveMacenski <https://github.com/SteveMacenski>_\n",
      "**Mentor contact details: [See link above, link in GitHub profile description]**\n",
      "**Project output requirements**\n",
      "- Integrations of 1 method of dynamic obstacle detection in ROS 2, using machine learning\n",
      "- Test these capabilities on real data or a live robot to demonstrate functionality\n",
      "- 85% test coverage or higher\n",
      "**Skills required**\n",
      "- C++, Python, ROS\n",
      "- Mobile robot navigation experience\n",
      "- Geometry and statistics\n",
      "- Recommended: Gazebo simulation, machine learning, ROS navigation\n",
      "**List of relevant open source software repositories and refs**\n",
      "- Starting project <https://github.com/ros-navigation/navigation2_dynamic/>_\n",
      "- ROS <https://www.ros.org/>_\n",
      "- Gazebo Simulator <http://gazebosim.org/>_\n",
      "- Github ticket <https://github.com/ros-navigation/navigation2/issues/1617>_\n",
      "- Navigation2 <https://docs.nav2.org/>_\n",
      "- Some related works <https://alyssapierson.files.wordpress.com/2018/05/pierson2018.pdf>_\n",
      "**Licensing**\n",
      "- All contributions will be under the Apache 2.0 license.\n",
      "- No other CLA's are required.\n"
     ]
    }
   ],
   "source": [
    "create_data_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_qdrant():\n",
    "    mongo_client = MongoClient(MONGO_URI)\n",
    "    db = mongo_client[DB_NAME]\n",
    "    collection = db[EMBEDDINGS_COLLECTION_NAME]\n",
    "    \n",
    "    records = collection.find()\n",
    "    count = collection.count_documents({})\n",
    "    qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance, for testing, CI/CD\n",
    "\n",
    "#Create a collection or database of texts where you store the embeddings\n",
    "    my_collection = \"text_collection\"\n",
    "\n",
    "    qdrant.create_collection(\n",
    "        collection_name=my_collection,\n",
    "        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)\n",
    "    )\n",
    "    i = 0\n",
    "    for record in records:\n",
    "        i+=1\n",
    "        source = record.get(\"source\")\n",
    "        url = record.get(\"url\")\n",
    "        data = record.get(\"data\")\n",
    "        embeddings = record.get(\"embeddings\")\n",
    "        \n",
    "        qdrant.upsert(\n",
    "        collection_name=my_collection,\n",
    "        points=[models.PointStruct(\n",
    "            id=i,\n",
    "            vector=embeddings,\n",
    "            payload={\"source\": source, \"url\": url, \"data\": data}\n",
    "            )]\n",
    "        )\n",
    "        print(\"Inserting data into qdrant for url:\", url)\n",
    "    \n",
    "    print(\"Number of data chunks stored into QDrant:\", count)\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
