{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from sentence_transformers import SentenceTransformer #TODO: 90% sure this is the best for embedding in our case, just check once\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL\n",
    "\n",
    "## Youtube ETL\n",
    "def get_youtube_transcript(video_url: str) -> list:\n",
    "    video_id = video_url.split(\"v=\")[1]\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    return transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Make a more genric function to store data into MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"youtube_database\"]  # Database name #TODO: Categorize better\n",
    "collection = db[\"transcripts\"]   # Collection name #TODO: Categorize better\n",
    "\n",
    "# Insert into MongoDB\n",
    "document = {\n",
    "    \"source\": \"youtube\",\n",
    "    \"video_id\": video_id,\n",
    "    \"transcript\": transcript\n",
    "}\n",
    "\n",
    "collection.insert_one(document)\n",
    "print(\"Transcript stored into MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "# Chunk sentences together for embedding\n",
    "#TODO: Make this chunking better:Maybe capitalize and fins end of sentences in a better way\n",
    "sentences_in_chunk = 6\n",
    "chunks = []\n",
    "for i in range(0, len(sentences), sentences_in_chunk):\n",
    "    chunks.append(\". \".join(sentences[i:i + sentences_in_chunk]))\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMbedding\n",
    "# Embed these chunks\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2') # TODO: Pick better models\n",
    "\n",
    "embeddings = model.encode(chunks)\n",
    "embeddings_dict = dict(zip(chunks, embeddings))\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store embeddings also in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant\n",
    "\n",
    "## Similarity search using qdrant\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance, for testing, CI/CD\n",
    "\n",
    "#Create a collection or database of texts where you store the embeddings\n",
    "my_collection = \"text_collection\"\n",
    "\n",
    "qdrant.create_collection(\n",
    "    collection_name=my_collection,\n",
    "    vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "#TODO: Make a function to insert embeddings into Qdrant\n",
    "# Insert embeddings into Qdrant\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "    qdrant.upsert(\n",
    "        collection_name=my_collection,\n",
    "        points=[models.PointStruct(\n",
    "            id=i,\n",
    "            vector=embedding,\n",
    "            payload={\"text\": chunk}\n",
    "        )]\n",
    "    )\n",
    "\n",
    "qdrant.count(\n",
    "    collection_name=my_collection,\n",
    "    exact=True,\n",
    ")\n",
    "\n",
    "#TODO: Write a fucntion to search for similar text\n",
    "sample_embedding = model.encode(sample_sentence)\n",
    "search_result = qdrant.search(\n",
    "    collection_name=my_collection,\n",
    "    query_vector=sample_embedding,\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM\n",
    "\n",
    "login() # Login to huggingface # TODO: Finalize whether this is required when you finalize the model\n",
    "\n",
    "use_quantization_config = True\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
    "\n",
    "#Instantiate tokenizer (tokenizer turns text into numbers ready for the model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "llm_model.to(\"cuda\")\n",
    "llm_model.eval() # put model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROS questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"WWhat are the main differences between ROS 1 and ROS 2?\",\n",
    "    \"Explain the concept of topics, services, and actions in ROS.\",\n",
    "    \"What is the purpose of the catkin_make command in ROS 1?\",\n",
    "    \"What is the function of rviz in ROS?\",\n",
    "    \"What is the purpose of rqt_graph, and how can it help in debugging a ROS system?\"\n",
    "] \n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"What is ros?\",\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items]) #TODO: This works, but maybe make it better?\n",
    "\n",
    "    system_message = f\"You are a robotic operating system (ROS) developer, using given context as additional information and answer the query, just your answer explanatory answer would suffice. Here is the context:{context}.\" # For out task we can use this as a system message\n",
    "\n",
    "    # Default prompt template\n",
    "    prompt=f'''<|im_start|>system\n",
    "    {system_message}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {query}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    search_results = qdrant.search(\n",
    "    collection_name=my_collection,\n",
    "    query_vector=model.encode(query),\n",
    "    limit=5\n",
    "    )\n",
    "    \n",
    "    # Create a list of context items\n",
    "    context_items = []\n",
    "    for result in search_results:\n",
    "        context_items.append({\"sentence_chunk\": result.payload['text']})\n",
    "\n",
    "    scores = [result.score for result in search_results]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i] # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 top_p=top_p,\n",
    "                                 top_k=top_k,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        prompt_format_matched = prompt.replace('<|im_start|>', '<|im_start|> ').replace('<|im_end|>\\n', '<|im_end|> \\n')\n",
    "        output_text = output_text.replace('<s>', '').replace(prompt_format_matched, '').replace('<|im_end|>', '')\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text, \"No context items returned\"\n",
    "    \n",
    "    return output_text, context_items\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing out everything\n",
    "\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
