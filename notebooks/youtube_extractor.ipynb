{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Yx104rVo_Dw\n",
      "[youtube] Yx104rVo_Dw: Downloading webpage\n",
      "[youtube] Yx104rVo_Dw: Downloading ios player API JSON\n",
      "[youtube] Yx104rVo_Dw: Downloading mweb player API JSON\n",
      "[youtube] Yx104rVo_Dw: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored data for video: Yx104rVo_Dw\n",
      "[{'url': 'https://www.youtube.com/watch?v=Yx104rVo_Dw', 'source': 'YouTube', 'video_id': 'Yx104rVo_Dw', 'transcript': \"hey my name is Dean my name is mati my name is Silva hi I'm Adam I started at Jan Street in January of 2020 since then machine learning has really taken off [Music] here so machine learning starts with the data in fact we approach uh Market data from a data science or machine learning perspective so at Jan Street we benefit from our Rich set of historical Market data it gives high resolution access to the events that went on on a particular exchange and we have that for dozens of exchanges globally and that set of data is really high quality and allows us to build really rich machine learning models that understand what happened on that Exchange in the ml team the data sets get pretty big our largest data sets get measured in the pedabytes but across the firm there's a huge dynamic range in other teams you might have a handful of observations ever sometimes zero obervations ever and there's a lot of care and subtlety and things to get right whether you're working with some huge custom neural network training on thousands of gpus or if you're fitting a model with one parameter when I started we were mostly doing stuff with linear models trying to fit them to different market conditions and roll them out into different trading systems since then our modeling work has become a lot more sophisticated and we've worked on lots of different model architectures so one thing uh that changes a lot compared to other places is that the market ch change uh when you interact with them and so a model that work one day can stop working a couple of months from now just because your competitors are doing something different or like some other Market participants have other strategies you constantly need to kind of revent yourself and find new ways to train new models to stay competitive machine learning is an important part of our business we have a team of folks who are machine learning engineers and machine learning researchers who are focused entirely on building new best-in-class models that we're going to use in trading applications this work wouldn't be possible without the work from dozens of other teams at Jan Street including our historical Market data group our realtime Market data group and then also our data center and networking folks I feel really lucky because a lot of my colleagues have invested a lot of time into making the research process as easy as possible it's really easy to collect data for example or to get resources to train my model I was actually very surprised at open uh the collaborative environment here uh is Traders will share free strategies with which is essential to train the best models because we need to understand how they're going to be used and the collaboration go both ways we also teach Traders how to use machine learning and train machine learning models themselves so that yeah we can develop the best strategies one of my favorite parts of my job is that I can go up to literally anyone and talk about what they're working on and pull in some of the smartest people I've ever met whenever I want I think this is our greatest superpower I think the real investment is the humans the talent density at Jan street is really high and the amount of brain power we're putting Behind these problems is huge CH stre deeply cares about ongoing education of all its employees we have like a boot camp we run every six months where we onboard newers and Traders uh on How to Train machine learning models and then for people who are more familiar with machine learning we have an ongoing ml papers Club where we take the recent papers that were just published and try to make sense of them together between Finance firms it's very competitive and it's really important to be at the limit of what's possible a lot of the problems that I work on feel like very new ground and i' surprised if we weren't some of the first people in history to be solving these problems we always want Curious smart and kind people from anywhere in the world at Jam Street we welcome candidates from all sort of background in particular we think it's really useful to have new hire from different background that can bring A New Perspective to existing problems some of the folks on our team had deep knowledge of computer vision coming in other folks knew a lot about NLP and some people knew a lot about reinforcement learning all those people contribute different areas to the team and sometimes they're working in a new area that's outside of their past research entirely something that is uh really exciting about machine learning in finance there are many areas that we haven't explored at all yet and I'm really excited to see how things will look like in like 3 years or something\", 'title': 'Meet the Machine Learning Team at Jane Street', 'description': 'Machine learning has been a key part of Jane Street’s work from the beginning; we’ve leveraged a variety of modeling techniques since our founding in 2000. Our Machine Learning team works to build and refine platforms and infrastructure that have a wide‑ranging impact on the firm. We’re always looking for smart, curious individuals to help us shape the future of machine learning at Jane Street.\\n\\nLearn more here: https://www.janestreet.com/machine-learning/', 'publishedAt': '2024-11-20T20:32:37Z', 'viewCount': '30150', 'likeCount': 0, 'commentCount': 0}]\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googleapiclient.discovery import build\n",
    "# from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "youtube_info = []\n",
    "# YouTube API setup\n",
    "YOUTUBE_API_KEY = 'AIzaSyDU3Qr_gjSK49dKVL5JDaFuzGfEjEKSGC4'\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "# MongoDB setup\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "# db = client['youtube_data']\n",
    "# collection = db['video_info']\n",
    "\n",
    "def extract_video_id(url):\n",
    "    with yt_dlp.YoutubeDL() as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        return info['id']\n",
    "\n",
    "def get_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        return ' '.join([entry['text'] for entry in transcript])\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting transcript for video {video_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_metadata(video_id):\n",
    "    try:\n",
    "        response = youtube.videos().list(\n",
    "            part='snippet,statistics',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        if 'items' in response:\n",
    "            video_data = response['items'][0]\n",
    "            snippet = video_data['snippet']\n",
    "            statistics = video_data['statistics']\n",
    "\n",
    "            return {\n",
    "                'title': snippet['title'],\n",
    "                'description': snippet['description'],\n",
    "                'publishedAt': snippet['publishedAt'],\n",
    "                'viewCount': statistics['viewCount'],\n",
    "                'likeCount': statistics.get('likeCount', 0),\n",
    "                'commentCount': statistics.get('commentCount', 0)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting metadata for video {video_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_video(url):\n",
    "    video_id = extract_video_id(url)\n",
    "    transcript = get_transcript(video_id)\n",
    "    metadata = get_metadata(video_id)\n",
    "\n",
    "    if transcript and metadata:\n",
    "        video_data = {\n",
    "            'url': url,\n",
    "            'source': 'YouTube',\n",
    "            'video_id': video_id,\n",
    "            'transcript': transcript,\n",
    "            **metadata\n",
    "        }\n",
    "        youtube_info.append(video_data)\n",
    "        # collection.insert_one(video_data)\n",
    "        print(f\"Processed and stored data for video: {video_id}\")\n",
    "    else:\n",
    "        print(f\"Failed to process video: {video_id}\")\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        process_video(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    youtube_urls = [\n",
    "        'https://www.youtube.com/watch?v=Yx104rVo_Dw'\n",
    "    ]\n",
    "    main(youtube_urls)\n",
    "    print(youtube_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
